{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3780e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "from IPython.display import Image, clear_output, display\n",
    "from picamera2 import Picamera2\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099e65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pyproject.toml in parent directories and add its directory to sys.path\n",
    "path_curr = Path.cwd()\n",
    "for parent in [path_curr] + list(path_curr.parents):\n",
    "    pyproject_file = parent / \"pyproject.toml\"\n",
    "    if pyproject_file.exists():\n",
    "        sys.path.insert(0, str(parent))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c1104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.camera.classes_camera import YOLOCameraManager\n",
    "from src.servo.classes_servo import DualServoController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1326c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA9685 initialized successfully\n",
      "PCA9685 initialized successfully\n",
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0:05:35.767427780] [2494] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:340 \u001b[0mlibcamera v0.6.0+rpt20251202\n",
      "[0:05:35.826108730] [2653] \u001b[1;32m INFO \u001b[1;37mIPAProxy \u001b[1;34mipa_proxy.cpp:180 \u001b[0mUsing tuning file /usr/share/libcamera/ipa/rpi/vc4/ov5647.json\n",
      "[0:05:35.840943602] [2653] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:223 \u001b[0mAdding camera '/base/soc/i2c0mux/i2c@1/ov5647@36' for pipeline handler rpi/vc4\n",
      "[0:05:35.841000899] [2653] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:445 \u001b[0mRegistered camera /base/soc/i2c0mux/i2c@1/ov5647@36 to Unicam device /dev/media0 and ISP device /dev/media2\n",
      "[0:05:35.841061289] [2653] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpipeline_base.cpp:1111 \u001b[0mUsing configuration file '/usr/share/libcamera/pipeline/rpi/vc4/rpi_apps.yaml'\n",
      "[0:05:35.866412834] [2494] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1215 \u001b[0mconfiguring streams: (0) 640x480-XRGB8888/sRGB (1) 640x480-SGBRG10_CSI2P/RAW\n",
      "[0:05:35.867342268] [2653] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:620 \u001b[0mSensor: /base/soc/i2c0mux/i2c@1/ov5647@36 - Selected sensor format: 640x480-SGBRG10_1X10/RAW - Selected unicam format: 640x480-pGAA/RAW\n"
     ]
    }
   ],
   "source": [
    "# initialize classes\n",
    "dual_servo_controller = DualServoController()\n",
    "yolo_camera_manager = YOLOCameraManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5f7bb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1mimgz\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['imgsz=640'].\n\n    Arguments received: ['yolo', '--f=/home/maasyuki/.config/Code/User/globalStorage/ms-toolsai.jupyter/version-2025.9.1/jupyter/runtime/kernel-v3b9b28954306b70d649b6282a37ace8ca97a4aeb9.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ['detect', 'segment', 'classify', 'obb', 'pose']\n                MODE (required) is one of ['benchmark', 'val', 'predict', 'track', 'export', 'train']\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Validate a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or any of ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92m~/Documents/2026/Projects/YOLO_on_Raspberry_Pi/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3701\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Cell \u001b[92mIn[5]\u001b[39m\u001b[92m, line 1\u001b[39m\n    yolo_camera_manager.display_results()\n",
      "  File \u001b[92m~/Documents/2026/Projects/YOLO_on_Raspberry_Pi/src/camera/classes_camera.py:67\u001b[39m in \u001b[95mdisplay_results\u001b[39m\n    results = self.get_capture_and_detect_results()\n",
      "  File \u001b[92m~/Documents/2026/Projects/YOLO_on_Raspberry_Pi/src/camera/classes_camera.py:57\u001b[39m in \u001b[95mget_capture_and_detect_results\u001b[39m\n    results = self._yolo_model.predict(image_bgr, imgz=self._imgz)\n",
      "  File \u001b[92m~/Documents/2026/Projects/YOLO_on_Raspberry_Pi/.venv/lib/python3.13/site-packages/ultralytics/engine/model.py:527\u001b[39m in \u001b[95mpredict\u001b[39m\n    self.predictor = (predictor or self._smart_load(\"predictor\"))(overrides=args, _callbacks=self.callbacks)\n",
      "  File \u001b[92m~/Documents/2026/Projects/YOLO_on_Raspberry_Pi/.venv/lib/python3.13/site-packages/ultralytics/engine/predictor.py:124\u001b[39m in \u001b[95m__init__\u001b[39m\n    self.args = get_cfg(cfg, overrides)\n",
      "  File \u001b[92m~/Documents/2026/Projects/YOLO_on_Raspberry_Pi/.venv/lib/python3.13/site-packages/ultralytics/cfg/__init__.py:310\u001b[39m in \u001b[95mget_cfg\u001b[39m\n    check_dict_alignment(cfg, overrides)\n",
      "\u001b[36m  \u001b[39m\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2026/Projects/YOLO_on_Raspberry_Pi/.venv/lib/python3.13/site-packages/ultralytics/cfg/__init__.py:505\u001b[39m\u001b[36m in \u001b[39m\u001b[35mcheck_dict_alignment\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mraise SyntaxError(string + CLI_HELP_MSG) from e\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>\u001b[39m\n\u001b[31m    \u001b[39m\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m '\u001b[31m\u001b[1mimgz\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['imgsz=640'].\n\n    Arguments received: ['yolo', '--f=/home/maasyuki/.config/Code/User/globalStorage/ms-toolsai.jupyter/version-2025.9.1/jupyter/runtime/kernel-v3b9b28954306b70d649b6282a37ace8ca97a4aeb9.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ['detect', 'segment', 'classify', 'obb', 'pose']\n                MODE (required) is one of ['benchmark', 'val', 'predict', 'track', 'export', 'train']\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Validate a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or any of ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "yolo_camera_manager.display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab5092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO_on_Raspberry_Pi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
